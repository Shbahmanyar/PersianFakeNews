{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persian Fake News Detection with Deep Learning\n",
    "### Author and Developer: üë©üèª‚Äçüíª Shahrzad Bahmanyar\n",
    "#### üìÖ February 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from transformers import TFBertModel\n",
    "import transformers\n",
    "from PersianFakeNews import PersianFakeNewsUtility\n",
    "from PersianFakeNewsConfig import PersianFakeNewsDetectionConfig\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility and Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Setting = {\n",
    "    \"Title\": \"Persian Fake News Detection\",\n",
    "    \"TaskName\": \"PersianFakeNewsDetection\",\n",
    "    \"Model\": \"Transformer\",\n",
    "    \"Number\": 8,\n",
    "    \"EmbeddingModel\": \"ParsBert\",\n",
    "    \"EmbeddingModel_Freeze\": False,\n",
    "    \"Epochs\": 500,\n",
    "    \"BatchSize\": 8,\n",
    "    \"Labels\": [\"offensive\", \"sentiment\", \"topic_assignment\", \"FAKE_label\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility and Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility = PersianFakeNewsUtility(Setting)\n",
    "modelConfig = PersianFakeNewsDetectionConfig(Setting[\"EmbeddingModel\"])\n",
    " \n",
    "labels = Setting[\"Labels\"]\n",
    "batch_size = Setting[\"BatchSize\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility.get_summary_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = utility.get_dataset(\"Train\", labels, batch_size)\n",
    "dev_dataset = utility.get_dataset(\"Dev\", labels, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = TFBertModel.from_pretrained(modelConfig.ModelName, trainable=False)\n",
    "input_ids = Input(shape=(280,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks = Input(shape=(280,), dtype=tf.int32, name='attention_masks')\n",
    "embedding_layer = bert_model(input_ids, attention_mask=attention_masks)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Model\n",
    "### Auxiliary model (topic modeling) to improve the performance of the main model (fake news detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Dense(280)(embedding_layer)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = tf.keras.activations.relu(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "x1 = Dense(64)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = tf.keras.activations.relu(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "out_offensive = Dense(2, activation='softmax', name=labels[0])(x1)\n",
    "\n",
    "x2 = Dense(280)(embedding_layer)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = tf.keras.activations.relu(x2)\n",
    "x2 = Dropout(0.2)(x2)\n",
    "x2 = Dense(64)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = tf.keras.activations.relu(x2)\n",
    "x2 = Dropout(0.2)(x2)\n",
    "out_sentiment = Dense(3, activation='softmax', name=labels[1])(x2)\n",
    "\n",
    "x3 = Dense(280)(embedding_layer)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = tf.keras.activations.relu(x3)\n",
    "x3 = Dropout(0.2)(x3)\n",
    "x3 = Dense(64)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = tf.keras.activations.relu(x3)\n",
    "x3 = Dropout(0.2)(x3)\n",
    "out_topic = Dense(10, activation='softmax', name=labels[2])(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake news Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(280)(embedding_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.activations.relu(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Concatenate()([x1, x2, x3, x])\n",
    "x = Dense(472)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.activations.relu(x)\n",
    "x = Dense(64)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.activations.relu(x)\n",
    "x = Dropout(0.2)(x)\n",
    "out_fake = Dense(2, activation='softmax', name = labels[3])(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_name = f\"{Setting['Number']}_{Setting['TaskName']}_{Setting['Model']}_{Setting['EmbeddingModel']}_Epochs{Setting['Epochs']}_Batchs{Setting['BatchSize']}\"\n",
    "model = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=[out_offensive, out_sentiment, out_topic, out_fake], name=_model_name)\n",
    "\n",
    "losses = {\n",
    "    labels[0]: SparseCategoricalCrossentropy(),\n",
    "    labels[1]: SparseCategoricalCrossentropy(),\n",
    "    labels[2]: SparseCategoricalCrossentropy(),\n",
    "    labels[3]: SparseCategoricalCrossentropy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "model.compile(optimizer='adam', loss=losses, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plot_path = f\"results/{Setting['Number']}_{Setting['TaskName']}_{Setting['Model']}_{Setting['EmbeddingModel']}_Epochs{Setting['Epochs']}_Batchs{Setting['BatchSize']}.png\"\n",
    "plot_model(\n",
    "    model,\n",
    "    to_file=model_plot_path,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = dev_dataset,\n",
    "    epochs = Setting[\"Epochs\"],\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "model_path = f\"models/{Setting['Number']}_{Setting['TaskName']}_{Setting['Model']}_{Setting['EmbeddingModel']}_Epochs{Setting['Epochs']}_Batchs{Setting['BatchSize']}.h5\"\n",
    "model.save(model_path)\n",
    "print(F\"‚úÖ Saved model to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(model_path, custom_objects={\"TFBertModel\": transformers.TFBertModel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Train History, accuracy and loss (train data and validation data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility.plot_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result and Plots Test Dataset (show and save) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility.generate_predictions(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
